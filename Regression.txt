


bin/hdfs dfs -mkdir p /ML/reg

bin/hdfs dfs -put /home/pratap/Documents/Study_Material/Spark/Machine_Learning_with_Spark/Data/Bike-Sharing-Dataset/hour_noheader.csv /ML/reg





sed 1d hour.csv > hour_noheader.csv

IPYTHON=1 IPYTHON_OPTS="-pylab" ./bin/pyspark --master yarn-client --executor-memory 2g

./bin/pyspark --master yarn-client --executor-memory 2g

raw_data = sc.textFile("/ML/reg/hour_noheader.csv")
records = raw_data.map(lambda x : x.split(","))
num_data = raw_data.count()
first = records.first()

print first
print num_data

records.cache()

def get_mapping(rdd,idx):
 return rdd.map(lambda fields:fields[idx]).distinct().zipWithIndex().collectAsMap()

print "Mapping of first categorical feature colum: %s"% get_mapping(records,2)


mappings = [get_mapping(records,i) for i in range(2,10)]

cat_len = sum(map(len,mappings))
num_len = len(records.first() [11:15])
total_len = cat_len+num_len


print "Feature vector length for categorical features: %d" % cat_len
print "Feature vector length for numerical features: %d" % num_len
print "Total feature vector length: %d" % total_len

from pyspark.mllib.regression import LabeledPoint
import numpy as np

def extract_features(record):
  cat_vec = np.zeros(cat_len)
  i = 0
  step = 0
  for field in record[2:9]:
    m = mappings[i]
    idx = m[field]
    cat_vec[idx+step]=1
    i = i+1
    step = step+len(m)
  num_vec = np.array([float(field) for field in record[10:14]])
  return np.concatenate((cat_vec,num_vec))

def extract_label(record):
  return float(record[-1]) 



data = records.map(lambda r: LabeledPoint(extract_label(r),extract_features(r)))

first_point = data.first()
print "Raw data: " + str(first[2:])
print "Label: "+str(first_point.label)
print "Linear Model feature vector: \n"+ str(first_point.features)
print "Linear Model feature vector length: " + str(len(first_point.features))


def extract_features_dt(record):
  return np.array(map(float,record[2:14]))
data_dt = records.map(lambda r: LabeledPoint(extract_label(r),extract_features_dt(r)))
first_point_dt = data_dt.first()
print "Decison Tree feature vector: "+ str(first_point_dt.features)
print "Decision Tree feature vector length :"+str(len(first_point_dt.features))

from pyspark.mllib.regression import LinearRegressionWithSGD
from pyspark.mllib.tree import DecisionTree

linear_model = LinearRegressionWithSGD.train(data, iterations = 10, step = 0.1, intercept = False)
true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))
print "Linear Model predictions : %s"%true_vs_predicted.take(5)


dt_model = DecisionTree.trainRegressor(data_dt,{})
preds = dt_model.predict(data_dt.map(lambda p:p.features))
actual = data.map(lambda p: p.label)
true_vs_predicted_dt = actual.zip(preds)
print "Decision Tree predictions:"+str(true_vs_predicted_dt.take(5))
print "Decision tree depth:"+str(dt_model.depth())
print "Decision tree number of nodes: "+str(dt_model.numNodes())


def squared_error(actual,pred):
  return (pred-actual)**2

def abs_error(actual,pred):
  return np.abs(pred-actual)

def squared_log_error(pred,actual):
  return (np.log(pred+1) - np.log(actual+1))**2

mse = true_vs_predicted.map(lambda (t,p):squared_error(t,p)).mean()
mae = true_vs_predicted.map(lambda (t,p):abs_error(t,p)).mean()
rmsle = np.sqrt(true_vs_predicted.map(lambda (t,p): squared_log_error(t,p)).mean())

print "Linear Model - Mean Squared Error: %2.4f" % mse
print "Linear Model - Mean Absolute Error: %2.4f" % mae
print "Linear Model - Root Mean Squared Log Error: %2.4f" % rmsle



mse_dt = true_vs_predicted_dt.map(lambda (t,p): squared_error(t,p)).mean()
mae_dt = true_vs_predicted_dt.map(lambda (t,p): abs_error(t,p)).mean()
rmsle_dt = true_vs_predicted_dt.map(lambda (t,p):squared_log_error(t,p)).mean()

print "Decision Tree - Mean Squared Error: %2.4f"%mse_dt
print "Decision Tree- Mean Absolute Error : %2.4f" % mae_dt
print "Decision Tree - Root Mean Squared Log Error: %2.4f" % rmsle_dt


targets = records.map(lambda r: float(r[-1])).collect()

import matplotlib.pyplot as plt
plt.hist(targets,bins=40, color = 'lightblue',normed=True)
plt.show()

log_targets = records.map(lambda r:np.log(float (r[-1]))).collect()
plt.hist(log_targets,bins=40,color='lightblue',normed=True)
plt.show()

sqrt_targets = records.map(lambda r: np.sqrt(float (r[-1]))).collect()
plt.hist(sqrt_targets,bins=40,color='lightblue',normed=True)
plt.show()


data_log = data.map(lambda lp: LabeledPoint(np.log(lp.label),lp.features))

model_log = LinearRegressionWithSGD.train(data_log,iterations=10,step = 0.1)

true_vs_predicted_log = data_log.map(lambda p: (np.exp(p.label),np.exp(model_log.predict(p.features))))

mse_log = true_vs_predicted_log.map(lambda (t,p):squarederror(t,p)).mean()
mae_log = true_vs_predicted_log.map(lambda(t,p):abs_error(t,p)).mean()
rmsle_log = true_vs_predicted_log.map(lambda(t,p):squared_log_error(t,p)).mean()

print "Mean Squared Error: %2.4f"%mse_log
print "Mean Absolute Error: %2.4f" %mae_log
print "Root mean Squared Log Error: %2.4f" %rmsle_log
print "Non log-transformed predictions: \n"+str(true_vs_predicted.take(3))
print "Log-transformed predictions:\n"+str(true_vs_predicted_log.take(3))

data_dt_log = data_dt.map(lambda lp:LabeledPoint(np.log(lp.label),lp.features))
dt_model_log = DecisionTree.trainRegressor(data_dt_log,{})

preds_log = dt_model_log.predict(data_dt_log.map(lambda p:p.features))
actual_log =data_dt_log.map(lambda p:p.label)
true_vs_predicted_dt_log = actual_log.zip(preds_log).map(lambda (t,p): (np.exp(t),np.exp(p)))

mse_log_dt = true_vs_predicted_dt_log.map(lambda (t,p): squared_error(t,p)).mean()
mae_log_dt = true_vs_predicted_dt_log.map(lambda (t,p):abs_error(t,p)).mean()
rmsle_log_dt = np.sqrt(true_vs_predicted_dt_log.map(lambda (t,p):squared_log_error(t,p)).mean())
print "Mean Squared Error: %2.4f" % mse_log_dt
print "Mean Absolute Error: %2.4f" % mae_log_dt
print "Root Mean Squared Log Error: %2.4f" % rmse_log_dt
print "Non log-tranformed predictions:\n"+str(true_vs_predicted_dt.take(3))
print "Log-transformed predictions:\n"+str(true_vs_predicted_dt_log.take(3))

data_with_idx = data.zipWithIndex().map(lambda (k,v):(v,k))
test = data_with_idx.sample(False,0.2,42)
train = data_with_idx.subtractByKey(test)


train_data = train.map(lambda (idx,p): p)
test_data = test.map(lambda (idx,p):p)
train_size = train_data.count()
test_size = test_data.count()
print "Training data size: %d" % train_size
print "Test data size: %d" % test_size
print "Total data size: %d" % num_data
print "Train + test size: %d" % (train_size+test_size)

data_with_idx_dt = data_dt.zipWithIndex().map(lambda (k,v):(v,k))
test_dt = data_with_idx_dt.sample(False,0.2,42)
train_dt = data_with_idx_dt.subtractByKey(test_dt)
train_data_dt = train_dt.map(lambda (idx,p):p)
test_data_dt = test_dt.map(lambda (idx,p):p)

def evaluate(train,test,iterations, step, regParam,regType,intercept):
  model = LinearRegressionWithSGD.train(train,iterations,step,regParam = regParam,regType = regType,intercept = intercept)
  tp = test.map(lambda p: (p.label,model.predict(p.features)))
  rmsle = np.sqrt(tp.map(lambda (t,p):squared_log_error(t,p)).mean())
  return rmsle


params = [1,5,10,20,50,100]
metrics = [evaluate(train_data,test_data,param,0.01,0.0,'l2',False) for param in params]
print params
print metrics
plt.plot(params,metrics)
plt.xscale('log')
plt.show()

params = [0.01,0.025,0.05,0.1,1.0]
metrics = [evaluate(train_data,test_data,10,param,0.0,'l2',False) for param in params]
print params
print metrics
plt.plot(params,metrics)
plt.axis([0,0.1,1.3,1.7])
plt.show()


params = [0.0,0.01,0.1,1.0,5.0,10.0,20.0]
metrics = [evaluate(train_data,test_data,10,0.1,param,'l2',False) for param in params]
print params
print metrics
plt.plot(params,metrics)
plt.xscale('log')
plt.axis([0,20.0,1.1,1.7])
plt.show()


params = [0.0,0.01,0.1,1.0,10.0,100.0,1000.0]
metrics = [evaluate(train_data,test_data,10,0.1,param,'l1',False) for param in params]
print params
print metrics
plt.plot(params,metrics)
plt.xscale('log')
plt.show()


model_l1 = LinearRegressionWithSGD.train(train_data,10,0.1,regParam=1.0,regType='l1',intercept=False)
model_l1_10 = LinearRegressionWithSGD.train(train_data,10,0.1,regParam=10.0,regType='l1',intercept=False)
model_l1_100 = LinearRegressionWithSGD.train(train_data,10,0.1,regParam=100.0,regType='l1',intercept=False)

print "L1 (1.0) number of zero weights:"+str(sum(model_l1.weights.array==0))
print "L1 (10.0) number of zeros weights: " + str(sum(model_l1_10.weights.array==0))
print "L1 (100.0) number of zeros weights: " + str(sum(model_l1_100.weights.array==0))



params = [False,True]
metrics = [evaluate(train_data,test_data,10,0.1,1.0,'l2',param) for param in params]
print params
print metrics
plt.bar(params,metrics, color='lightblue')
plt.show()

def evaluate_dt(train,test,maxDepth,maxBins):
  model = DecisionTree.trainRegressor(train,{},impurity = 'variance',maxDepth=maxDepth,maxBins=maxBins)
  preds = model.predict(test.map(lambda p:p.features))
  actual = test.map(lambda p:p.label)
  tp = actual.zip(preds)
  rmsle = np.sqrt(tp.map(lambda(t,p):squared_log_error(t,p)).mean())
  return rmsle


params = [1,2,3,4,5,10,20]
metrics = [evaluate_dt(train_data_dt,test_data_dt,param,32) for param in params]
print params
print metrics
plt.plot(params,metrics)
plt.show()


params = [2,4,8,16,32,64,100]
metrics = [evaluate_dt(train_data_dt,test_data_dt,5,param) for param in params]
print params
print metrics
plt.plot(params,metrics)
plt.show()








































































